{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Calculus is the study of continuous change. It has two major branches, differential calculus and integral calculus. The former concerns instantaneous rates of change (derivatives), while the latter concerns accumulation of quantities (integrals).\n",
    "\n",
    "Both branches are relevant to machine learning. For example, the derivative of a function is used to find the slope of a line tangent to a curve. This is useful for finding the minimum of a function, which is a common task in machine learning. The integral of a function is used to find the area under a curve. This is useful for finding the probability of an event occurring, which is also a common task in machine learning.\n",
    "\n",
    "## Limits\n",
    "\n",
    "The limit of a function $f(x)$ as $x$ approaches $a$ is the value that $f(x)$ approaches as $x$ gets closer and closer to $a$. It is written as:\n",
    "\n",
    "$$\\lim_{x \\to a} f(x)$$\n",
    "\n",
    "For example, the limit of $f(x) = x^2$ as $x$ approaches $2$ is $4$:\n",
    "\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "The derivative of a function $f(x)$ is defined as the limit of the difference quotient as $\\Delta x$ approaches zero:\n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$\n",
    "\n",
    "## Integrals\n",
    "\n",
    "The integral of a function $f(x)$ is defined as the limit of the Riemann sum as $n$ approaches infinity:\n",
    "\n",
    "$$\\int_a^b f(x) dx = \\lim_{n \\to \\infty} \\sum_{i=1}^n f(x_i) \\Delta x$$\n",
    "\n",
    "\n",
    " -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Common Derivatives\n",
    "\n",
    "| Function | Derivative |\n",
    "| --- | --- |\n",
    "| $c$ | $0$ |\n",
    "| $x$ | $1$ |\n",
    "| $x^n$ | $nx^{n-1}$ |\n",
    "| $\\sqrt{x}$ | $\\frac{1}{2\\sqrt{x}}$ |\n",
    "| $e^x$ | $e^x$ |\n",
    "| $a^x$ | $a^x \\ln a$ |\n",
    "| $\\ln x$ | $\\frac{1}{x}$ |\n",
    "| $\\log_a x$ | $\\frac{1}{x \\ln a}$ |\n",
    "| $\\sin x$ | $\\cos x$ |\n",
    "| $\\cos x$ | $-\\sin x$ |\n",
    "| $\\tan x$ | $\\sec^2 x$ |\n",
    "| $\\sec x$ | $\\sec x \\tan x$ |\n",
    "| $\\cot x$ | $-\\csc^2 x$ |\n",
    "| $\\csc x$ | $-\\csc x \\cot x$ |\n",
    "\n",
    "## Common Integrals\n",
    "\n",
    "| Function | Integral |\n",
    "| --- | --- |\n",
    "| $x^n$ | $\\frac{x^{n+1}}{n+1} + C$ |\n",
    "| $\\frac{1}{x}$ | $ ln \\|x\\| + C$ |\n",
    "| $\\frac{1}{x^2 + 1}$ | $\\arctan x + C$ |\n",
    "| $e^x$ | $e^x + C$ |\n",
    "| $\\frac{1}{x \\ln a}$ | $ log_a \\|\\ln x\\| + C$ |\n",
    "| $\\sin x$ | $-\\cos x + C$ |\n",
    "| $\\cos x$ | $\\sin x + C$ |\n",
    "| $\\sec^2 x$ | $\\tan x + C$ |\n",
    "| $\\sec x \\tan x$ | $\\sec x + C$ |\n",
    "| $\\csc^2 x$ | $-\\cot x + C$ |\n",
    "| $\\csc x \\cot x$ | $-\\csc x + C$ |\n",
    "\n",
    "## Cost Functions\n",
    "\n",
    "A cost function is a function that measures the performance of a machine learning model. It is used to find the parameters of the model that minimize the cost function. The cost function is also known as the loss function or the error function.\n",
    "\n",
    "The cost function is usually written as $J(\\theta)$, where $\\theta$ is a vector of parameters. For example, in linear regression, $\\theta$ is a vector of weights. The cost function is a function of the training data and the parameters:\n",
    "\n",
    "$$J(\\theta) = J(X, y, \\theta)$$\n",
    "\n",
    "The cost function is a measure of how well the model fits the training data. It is usually defined as the sum of the squared errors between the model's predictions and the actual values:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "The cost function is a measure of how well the model fits the training data. It is usually defined as the sum of the squared errors between the model's predictions and the actual values:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Other cost functions are used for other types of machine learning models. For example, the cost function for logistic regression is the negative log-likelihood:\n",
    "\n",
    "$$J(\\theta) = -\\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]$$\n",
    " -->\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
