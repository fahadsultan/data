{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors \n",
    "\n",
    "## Scalars\n",
    "\n",
    "Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values **scalars**. \n",
    "\n",
    "For example, the temperature in Greenville is a balmy $72$ degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression $c = \\frac{5}{9}(f - 32)$, setting $f = 72$. In this equation, the values $5$, $9$, and $32$ are _constant scalars_. The variables $c$ and $f$ in general represent _unknown scalars_.\n",
    "\n",
    "We denote scalars by ordinary lower-cased letters (e.g. $x$, $y$, and $z$) and the space of all (continuous) _real-valued_ scalars by $\\mathbb{R}$. The expression $x \\in \\mathbb{R}$ is a formal way to say that $x$ is a real-valued scalar. The symbol $\\in$ (pronounced “in”) denotes membership in a set. For example, $x, y \\in {0, 1}$ indicates that $x$ and $y$ are variables that can only take on values of $0$ or $1$.\n",
    "\n",
    "Scalars in Python are represented by numeric types such as `int` and `float`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3\n",
    "y = 2\n",
    "\n",
    "print(\"x+y:\", x+y, \"x-y:\", x-y, \"x*y:\", x*y, \"x/y:\", x/y, \"x**y:\", x**y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For current purposes, you can think of a vector as a fixed-length array of scalars. As with their code counterparts, we call these scalars the elements of the vector (synonyms include entries and components). When vectors represent examples from real-world datasets, their values hold some real-world significance. For example, if we were training a model to predict the risk of a loan defaulting, we might associate each applicant with a vector whose components correspond to quantities like their income, length of employment, or number of previous defaults. If we were studying the risk of heart attack, each vector might represent a patient and its components might correspond to their most recent vital signs, cholesterol levels, minutes of exercise per day, etc. We denote vectors by bold lowercase letters, (e.g., \n",
    "$\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$).\n",
    "\n",
    "Vectors are implemented in Python as `list` or `tuples`. In pandas, we have for vectors `pd.Series` which additionally has labels for each value. In general, such `pd.Series` can have arbitrary lengths, subject to memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10\n",
       "1    20\n",
       "2    30\n",
       "3    40\n",
       "4    50\n",
       "5    60\n",
       "6    70\n",
       "7    80\n",
       "8    90\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "x = pd.Series(range(10, 100, 10))\n",
    "\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Alternative interpretation of vectors\n",
    "\n",
    "A vector is a quantity that has both magnitude and direction. Vectors are often represented as arrows in a coordinate system. The length of the arrow represents the magnitude of the vector, and the direction of the arrow represents the direction of the vector.\n",
    "\n",
    "Vectors are often used to represent physical quantities such as velocity, acceleration, and force. For example, the velocity of a car can be represented as a vector with a magnitude of 60 miles per hour and a direction of east.\n",
    "\n",
    "Vectors can be added together to form new vectors. For example, the velocity of a car can be added to the velocity of another car to form the velocity of a third car. Vectors can also be multiplied by scalars to form new vectors. For example, the velocity of a car can be multiplied by 2 to form the velocity of a car that is twice as fast. -->\n",
    "\n",
    "\n",
    "### Geometry of Vectors \n",
    "\n",
    "We can refer to an element of a vector by using a subscript. For example, $x_2$ denotes the second element of $\\mathbf{x}$. Since $x_2$ is a scalar, we do not bold it. By default, we visualize vectors by stacking their elements vertically.\n",
    "\n",
    "```{admonition} 0-based indexing vs. 1-based indexing\n",
    ":class: warning\n",
    "\n",
    "In Python, as in most programming languages, vector indices start at $0$. This is known as zero-based indexing. \n",
    "\n",
    "In linear algebra, however, subscripts begin at $1$ (one-based indexing).\n",
    "```\n",
    "\n",
    "A **vector** $\\mathbf{x} ∈ \\mathbb{R}^n$ is a list of $n$ numbers, usually written as a **column vector**\n",
    "\n",
    "$$ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$\n",
    "\n",
    "Here $x_1 \\ldots x_n $ are elements of the vector. Later on, we will distinguish between such **column vectors** and **row vectors** whose elements are stacked horizontally. Recall that we access a tensor’s elements via indexing.\n",
    "\n",
    "The vector of all ones is denoted $\\mathbf{1}$. The vector of all zeros is denoted $\\mathbf{0}$. \n",
    "\n",
    "The **unit vector $e_i$** is a vector of all 0’s, except entry $i$, which has value 1:\n",
    "\n",
    "$$ e_i = (0, \\ldots 0, 1, 0, \\ldots 0) $$\n",
    "\n",
    "This is also called a **one-hot vector**.\n",
    "\n",
    "First, we need to discuss the two common geometric interpretations of vectors, as either points or directions in space. Fundamentally, a vector is a list of numbers such as the Python list below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 7, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematicians most often write this as either a column or row vector, which is to say either as\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix}1\\\\7\\\\0\\\\1\\end{bmatrix}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\mathbf{x}^\\top = \\begin{bmatrix}1 & 7 & 0 & 1\\end{bmatrix}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These often have different interpretations, where data examples are column vectors and weights used to form weighted sums are row vectors. However, it can be beneficial to be flexible. \n",
    "\n",
    "Even though a single vector’s default orientation is a column vector, **for any matrix representing a tabular dataset, treating each data example as a row vector in the matrix is more conventional**.\n",
    "\n",
    "Given a vector, the first interpretation that we should give it is as a point in space. In two or three dimensions, we can visualize these points by using the components of the vectors to define the location of the points in space compared to a fixed reference called the origin. This can be seen in the figure below. \n",
    "\n",
    "An illustration of visualizing vectors as points in the plane. The first component of the vector gives the $x$-coordinate, the second component gives the $y$-coordinate. Higher dimensions are analogous, although much harder to visualize.\n",
    "\n",
    "``` {figure} https://d2l.ai/_images/grid-points.svg\n",
    "---\n",
    "width: 60%\n",
    "name: grid-points\n",
    "---\n",
    "An illustration of visualizing vectors as points in the plane. The first component of the vector gives the $x$-coordinate, the second component gives the $y$-coordinate. Higher dimensions are analogous, although much harder to visualize.\n",
    "```\n",
    "\n",
    "<!-- <center><img  width=\"60%\" src=\"https://d2l.ai/_images/grid-points.svg\"></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This geometric point of view allows us to consider the problem on a more abstract level. No longer faced with some insurmountable seeming problem like classifying pictures as either cats or dogs, we can start considering tasks abstractly as collections of points in space and picturing the task as discovering how to separate two distinct clusters of points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parallel, there is a second point of view that people often take of vectors: as directions in space. Not only can we think of the vector $\\textbf{v} = [3, 2]^{T}$ as the location $3$ units to the right and \n",
    "$2$ units up from the origin, we can also think of it as the direction itself to take $3$ steps to the right and $2$ steps up. In this way, we consider all the vectors in figure below the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {figure} https://d2l.ai/_images/par-vec.svg\n",
    "---\n",
    "width: 50%\n",
    "name: par-vec\n",
    "---\n",
    "Any vector can be visualized as an arrow in the plane. In this case, every vector drawn is a representation of the vector $(3, 2)^\\top$\n",
    "```\n",
    "\n",
    "<!-- <center><img width=\"50%\" src=\"https://d2l.ai/_images/par-vec.svg\"></center> -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of this shift is that we can make visual sense of the act of vector addition. In particular, we follow the directions given by one vector, and then follow the directions given by the other, as seen below: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {figure} https://d2l.ai/_images/vec-add.svg\n",
    "---\n",
    "width: 50%\n",
    "name: vec-add\n",
    "---\n",
    "We can visualize vector addition by first following one vector, and then another.\n",
    "```\n",
    "\n",
    "<!-- <center><img width=\"50%\" src=\"https://d2l.ai/_images/vec-add.svg\"></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector subtraction has a similar interpretation. By considering the identity that $\\mathbf{u} = \\mathbf{v} + (\\mathbf{u} - \\mathbf{v})$, we see that the vector $\\mathbf{u} - \\mathbf{v}$ is the direction that takes us from the point $\\mathbf{v}$ to the point $\\mathbf{u}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most fundamental operations in linear algebra (and all of data science and machine learning) is the dot product. \n",
    "\n",
    "Given two vectors $\\textbf{x}, \\textbf{y} \\in \\mathbb{R}^d$, their _dot product_ $\\textbf{x}^{\\top} \\textbf{y}$ (also known as _inner product_ $\\langle \\textbf{x}, \\textbf{y} \\rangle$) is a sum over the products of the elements at the same position: \n",
    "\n",
    "$$\\textbf{x}^\\top \\textbf{y} = \\sum_{i=1}^{d} x_i y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "y = pd.Series([4, 5, 6])\n",
    "\n",
    "x.dot(y) # 1*4 + 2*5 + 3*6 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can calculate the dot product of two vectors by performing an elementwise multiplication followed by a sum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x * y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector $ \\mathbf{x} \\in \\mathbb{R}^{n} $ , and a set of weights, denoted by $\\mathbf{x} \\in \\mathbb{R}^{n}$, the weighted sum of the values in $\\mathbf{x}$ according to the weights $\\mathbf{w}$ could be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}$. When the weights are nonnegative and sum to $1$, i.e., $(\\sum_{i=1}^n w_i = 1)$, the dot product expresses a _weighted average_. After normalizing two vectors to have unit length, the dot products express the cosine of the angle between them. Later in this section, we will formally introduce this notion of _length_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms \n",
    "\n",
    "Some of the most useful operators in linear algebra are _norms_. Informally, the norm of a vector tells us how _big_ it is. \n",
    "\n",
    "For instance, the $l_2$ norm measures the (Euclidean) length of a vector. \n",
    "\n",
    "Here, we are employing a notion of _size_ that concerns the magnitude of a vector’s components (not its dimensionality).\n",
    "\n",
    "A norm is a function $\\| \\cdot \\|$ that maps a vector to a scalar and satisfies the following three properties:\n",
    "\n",
    "1. Given any vector $\\mathbf{x}$, if we scale (all elements of) the vector by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n",
    "\n",
    "$$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|$$\n",
    "\n",
    "2. For any vectors $\\mathbf{x}$ and $\\mathbf{y}$, norms satisfy the triangle inequality:\n",
    "$$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|$$\n",
    "\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n",
    "\n",
    "$$ \\|\\mathbf{x}\\| > 0 \\textrm{ for all } \\mathbf{x} \\neq 0. $$\n",
    "\n",
    "Many functions are valid norms and different norms encode different notions of size. The Euclidean norm that we all learned in elementary school geometry when calculating the hypotenuse of a right triangle is the square root of the sum of squares of a vector’s elements. Formally, this is called the $l_2$ norm and expressed as\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l2_norm = (x**2).sum()**(1/2)\n",
    "l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $l_1$ norm is also common and the associated measure is called the Manhattan distance. By definition, the $l_1$ norm sums the absolute values of a vector’s elements:\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right| $$\n",
    "\n",
    "Compared to the $l_2$ norm, it is less sensitive to outliers. To compute the $l_1$ norm, we compose the absolute value with the sum operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l1_norm = x.abs().sum()\n",
    "l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the $l_1$ and $l_2$ norms are special cases of the more general norms:\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}. $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products and Angles\n",
    "\n",
    "If we take two column vectors $\\mathbf{u}$ and $\\mathbf{v}$, we can form their dot product by computing:\n",
    "\n",
    "$$ \\mathbf{u}^\\top\\mathbf{v} = \\sum_i u_i\\cdot v_i $$\n",
    "\n",
    "Because the equation above is symmetric, we will mirror the notation of classical multiplication and write\n",
    "\n",
    "$$ \\mathbf{u}\\cdot\\mathbf{v} = \\mathbf{u}^\\top\\mathbf{v} = \\mathbf{v}^\\top\\mathbf{u}, $$\n",
    "\n",
    "to highlight the fact that exchanging the order of the vectors will yield the same answer.\n",
    "\n",
    "\n",
    "The dot product also admits a geometric interpretation: dot product it is closely related to the angle between two vectors. \n",
    "\n",
    "``` {figure} https://d2l.ai/_images/vec-angle.svg\n",
    "---\n",
    "width: 50%\n",
    "name: vec-angle\n",
    "---\n",
    "Between any two vectors in the plane there is a well defined angle $\\theta$. We will see this angle is intimately tied to the dot product.\n",
    "```\n",
    "\n",
    "To start, let’s consider two specific vectors:\n",
    "\n",
    "$$ \\mathbf{v} = (r,0) \\; \\textrm{and} \\; \\mathbf{w} = (s\\cos(\\theta), s \\sin(\\theta)) $$\n",
    "\n",
    "The vector $\\mathbf{v}$ is length $r$ and runs parallel to the $x$-axis, and the vector $mathbf{w}$ is of length $s$ and at angle $\\theta$ with the $x$-axis. \n",
    "\n",
    "If we compute the dot product of these two vectors, we see that\n",
    "\n",
    "$$ \\mathbf{v}\\cdot\\mathbf{w} = rs\\cos(\\theta) = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos(\\theta) $$\n",
    "\n",
    "With some simple algebraic manipulation, we can rearrange terms to obtain <u>the equation for any two vectors $\\mathbf{v}$ and $\\mathbf{w}$</u>:\n",
    "\n",
    "$$ \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right) $$\n",
    "\n",
    "We will not use it right now, but it is useful to know that we will refer to vectors for which the angle is $\\pi/2$(or equivalently $90^{\\circ}$) as being orthogonal. \n",
    "\n",
    "By examining the equation above, we see that this happens when $\\theta = \\pi/2$, which is the same thing as $cos(\\theta) = 0$. \n",
    "\n",
    "The only way this can happen is if the dot product itself is zero, and two vectors are orthogonal if and only if $\\mathbf{v}\\cdot\\mathbf{w} = 0$. \n",
    "\n",
    "This will prove to be a helpful formula when understanding objects geometrically.\n",
    "\n",
    "It is reasonable to ask: why is computing the angle useful? Consider the problem of classifying text data. We might want the topic or sentiment in the text to not change if we write twice as long of document that says the same thing. \n",
    "\n",
    "For some encoding (such as counting the number of occurrences of words in some vocabulary), this corresponds to a doubling of the vector encoding the document, so again we can use the angle.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "In ML contexts where the angle is employed to measure the closeness of two vectors, practitioners adopt the term cosine similarity to refer to the portion\n",
    "\n",
    "$$ \\cos(\\theta) = \\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}. $$\n",
    "\n",
    "The cosine takes a maximum value of $1$ when the two vectors point in the same direction, a minimum value of $-1$ when they point in opposite directions, and a value of $0$\n",
    " when the two vectors are orthogonal. \n",
    " \n",
    "Note that if the components of high-dimensional vectors are sampled randomly with mean $0$, their cosine will nearly always be close to $0$.\n",
    "\n",
    "### Hyperplanes \n",
    "\n",
    "In addition to working with vectors, another key object that you must understand to go far in linear algebra is the hyperplane, a generalization to higher dimensions of a line (two dimensions) or of a plane (three dimensions). In an \n",
    "-dimensional vector space, a hyperplane has \n",
    " dimensions and divides the space into two half-spaces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with an example. Suppose that we have a column vector $\\mathbf{w}=[2,1]^\\top$. We want to know, “what are the points $\\mathbf{v}$ with $\\mathbf{w}\\cdot\\mathbf{v} = 1$?” \n",
    "\n",
    "By recalling the connection between dot products and angles above  we can see that this is equivalent to\n",
    "\n",
    "$$ \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos(\\theta) = 1 \\; \\iff \\; \\|\\mathbf{v}\\|\\cos(\\theta) = \\frac{1}{\\|\\mathbf{w}\\|} = \\frac{1}{\\sqrt{5}}. $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center>\n",
    "<img src=\"https://d2l.ai/_images/proj-vec.svg\">\n",
    "</center> -->\n",
    "\n",
    "``` {figure} https://d2l.ai/_images/proj-vec.svg\n",
    "---\n",
    "width: 50%\n",
    "name: proj-vec\n",
    "---\n",
    "Recalling trigonometry, we see the formula $\\|\\mathbf{v}\\|\\cos(\\theta)$ is the length of the projection of the vector $\\mathbf{v}$ onto the direction of $\\mathbf{w}$\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the geometric meaning of this expression, we see that this is equivalent to saying that the length of the projection of $\\mathbf{v}$ onto the direction of \n",
    "$\\mathbf{w}$ is exactly $1/\\|\\mathbf{w}\\|$, as is shown in the figure above. \n",
    "\n",
    "The set of all points where this is true is a line at right angles to the vector $\\mathbf{w}$. \n",
    "\n",
    "If we wanted, we could find the equation for this line and see that it is $2x + y = 1$ or equivalently $y = 1 - 2x$.\n",
    "\n",
    "If we now look at what happens when we ask about the set of points with $\\mathbf{w}\\cdot\\mathbf{v} > 1$ or $\\mathbf{w}\\cdot\\mathbf{v} < 1$ we can see that these are cases where the projections are longer or shorter than $1/\\|\\mathbf{w}\\|$, respectively.\n",
    "\n",
    "Thus, those two inequalities define either side of the line. \n",
    "\n",
    "In this way, we have found a way to cut our space into two halves, where all the points on one side have dot product below a threshold, and the other side above as we see below:\n",
    "\n",
    "``` {figure} https://d2l.ai/_images/space-division.svg\n",
    "---\n",
    "width: 50%\n",
    "name: space-division\n",
    "---\n",
    "If we now consider the inequality version of the expression, we see that our hyperplane (in this case: just a line) separates the space into two halves.\n",
    "```\n",
    "\n",
    "The story in higher dimension is much the same. If we now take $\\mathbf{w} = [1,2,3]^\\top$ and ask about the points in three dimensions with $\\mathbf{w}\\cdot\\mathbf{v} = 1$, we obtain a plane at right angles to the given vector $\\mathbf{w}$. The two inequalities again define the two sides of the plane as is shown below: \n",
    "\n",
    "\n",
    "``` {figure} https://d2l.ai/_images/space-division-3d.svg\n",
    "---\n",
    "width: 50%\n",
    "name: space-division-3d\n",
    "---\n",
    "Hyperplanes in any dimension separate the space into two halves.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our ability to visualize runs out at this point, nothing stops us from doing this in tens, hundreds, or billions of dimensions. This occurs often when thinking about machine learned models. \n",
    "\n",
    "For instance, we can understand linear classification models, as methods to find hyperplanes that separate the different target classes. In this context, such hyperplanes are often referred to as decision planes. The majority of deep learned classification models end with a linear layer fed into a softmax, so one can interpret the role of the deep neural network to be to find a non-linear embedding such that the target classes can be separated cleanly by hyperplanes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
