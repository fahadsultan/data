{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input.\n",
    "\n",
    "One method for classification is to use handwritten rules. There are many areas of data mining where handwritten rule-based classifiers constitute a state-of-the-art system, or at least part of it. \n",
    "Rules can be fragile, however, as situations or data change over time, and for some tasks [humans arenâ€™t necessarily good at coming up with the rules](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf). Most cases of classification therefore are instead done via supervised machine learning.\n",
    "\n",
    "\n",
    "**Classification** is the type of supervised learning where **$\\mathcal{y}$ is a <a src=\"../1_programming/41_types.html#categorical-features\">discrete categorical variable</a>**. \n",
    "\n",
    "The discrete output variable $\\mathcal{y}$ is often also called the **label** or **target** or **class**.\n",
    "\n",
    "For example, we might want to predict whether a patient has a disease or not, based on their symptoms. In this case, $\\mathcal{y}$ is a binary variable, taking the value 1 if the patient has the disease, and 0 otherwise. Other examples of classification problems include predicting the sentiment of a movie review: positive, negative, or neutral.\n",
    "\n",
    "For example,\n",
    "\n",
    "<img align=\"center\" width=\"90%\" src=\"../assets/sentiment.png\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "In other words, the classification problem is to learn a function $f$ that maps the input $\\mathcal{X}$ to the discrete output $\\mathcal{Y}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "The most common metric for evaluating a classifier is **accuracy**. Accuracy is the proportion of correct predictions. It is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "$$Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%.\n",
    "\n",
    "Accuracy is a good metric when the classes are _balanced_ $N_{class1} \\approx N_{class2}$. However, when the classes are imbalanced, accuracy can be misleading. For example, if we have a test set of 100 documents, and 95 of them are positive and 5 of them are negative, then a classifier that always predicts positive will have an accuracy of 95%. However, this classifier is not useful, because it never predicts negative.\n",
    "\n",
    "### Multi-class classification as multiple Binary classifications\n",
    "\n",
    "Every multi-class classification problem can be decomposed into multiple binary classification problems. For example, if we have a multi-class classification problem with 3 classes, we can decompose it into 3 binary classification problems.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../assets/binary_multiclass.png\">\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Assuming the categorical variable that we are trying to predict is binary, we can define the accuracy in terms of the four possible outcomes of a binary classifier: \n",
    "\n",
    "1. True Positive (TP): The classifier correctly predicted the positive class.\n",
    "2. False Positive (FP): The classifier **incorrectly** predicted the negative class as positive.\n",
    "3. True Negative (TN): The classifier correctly predicted the negative class.\n",
    "4. False Negative (FN):  The classifier **incorrectly** predicted the positive class as negative.\n",
    "\n",
    "True positive means that the classifier correctly predicted the positive class. False positive means that the classifier incorrectly predicted the positive class. True negative means that the classifier correctly predicted the negative class. False negative means that the classifier incorrectly predicted the negative class.\n",
    "\n",
    "These definitions are summarized in the table below: \n",
    "\n",
    "|       | Prediction $\\hat{y} = f'(x)$ | Truth $y = f(x)$     |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| True Negative (TN)    | 0        | 0   |\n",
    "| False Negative (FN)   | 0        | 1      |\n",
    "| False Positive (FP)   | 1        | 0      |\n",
    "| True Positive (TP)   | 1        | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the four outcomes above, the accuracy is:\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Accuracy is a useful metric, but it can be misleading. \n",
    "\n",
    "Other metrics that are often used to evaluate classifiers are: \n",
    "\n",
    "* **Precision**: The proportion of positive predictions that are correct. Mathematically, it is defined as:\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**: The proportion of positive instances that are correctly predicted. Mathematically, it is defined as:\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "The precision and recall are often combined into a single metric called the **F1 score**. The F1 score is the harmonic mean of precision and recall. The harmonic mean of two numbers is given by:\n",
    "\n",
    "* **F1 Score**: The harmonic mean of precision and recall.\n",
    "\n",
    "$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "<!-- $$Baseline\\ Accuracy = \\frac{Number\\ of\\ majority\\ class\\ predictions}{Total\\ number\\ of\\ predictions}$$ -->\n",
    "\n",
    "<!-- The baseline accuracy is the accuracy of the majority class classifier. It is the accuracy we would get if we just guessed the majority class for every instance. It is a useful baseline to compare our classifier to. If our classifier is not better than the baseline, then we should probably just use the baseline classifier.\n",
    "\n",
    "Another way to evaluate a classifier is to look at the confusion matrix. A confusion matrix is a table that shows the number of correct and incorrect predictions for each class. For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%. But if we had just guessed the majority class for all of them, we would have gotten 50% accuracy. This is called the baseline accuracy.\n",
    "\n",
    "<img src=\"../assets/confusion_matrix.png\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/classification.png\">\n",
    "\n",
    "\n",
    "<img src=\"../assets/cross_validation.png\">\n",
    "\n",
    "\n",
    "<img src=\"../assets/training_testing.png\">\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Many kinds of machine learning algorithms are used to build classifiers. Two common classifiers are **Naive Bayes** and **Logistic Regression**. \n",
    "\n",
    "These exemplify two primary category of models for doing classification:\n",
    "\n",
    "1. **Generative models** like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation.\n",
    "\n",
    "2. **Discriminative models** like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. \n",
    "\n",
    "While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. They can be more robust to missing data, and can be used to generate synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "\n",
    "The code below shows how to train a Nearest Neighbor classifier on the Iris dataset. The Iris dataset is a dataset of 150 observations of iris flowers. There are 3 classes of iris flowers: setosa, versicolor, and virginica. For each observation, there are 4 features: sepal length, sepal width, petal length, and petal width. The goal is to predict the class of iris flower given the 4 features.\n",
    "\n",
    "The code below uses the [scikit-learn](https://scikit-learn.org/stable/) library to train a Nearest Neighbor classifier on the Iris dataset. The Nearest Neighbor classifier is a simple classifier that works by finding the training observation that is closest to the test observation, and predicting the class of the closest training observation. The Nearest Neighbor classifier is a discriminative classifier.\n",
    "\n",
    "There are 5 steps shown in the code below:\n",
    "\n",
    "1. **Import the dataset**: The Iris dataset is included in scikit-learn. We import it using the `load_iris` function.\n",
    "2. **Split the dataset into training and test sets**: We split the dataset into a training set and a test set. The training set is used to train the classifier, and the test set is used to evaluate the classifier.\n",
    "3. **Instantiate the classifier**: We instantiate the classifier using the `KNeighborsClassifier` class.\n",
    "4. **Train the classifier**: We train the classifier using the `fit` method.\n",
    "5. **Make predictions**: We make predictions on the test set using the `predict` method.\n",
    "6. **Print the classification report**: We evaluate the classifier using the `classification_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       0.91      1.00      0.95        10\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load the data\n",
    "data = load_iris(as_frame=True)\n",
    "X    = data['data']\n",
    "y    = data['target']\n",
    "\n",
    "# 2. Create a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 3. Instantiate a model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# 4. Fit a model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict on the test set\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# 6. Print classification report\n",
    "print(classification_report(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
