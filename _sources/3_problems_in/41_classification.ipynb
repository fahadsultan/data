{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input.\n",
    "\n",
    "**Classification** is the type of supervised learning where **$\\mathcal{y}$ is a <a src=\"../1_programming/41_types.html#categorical-features\">discrete categorical variable</a>**. \n",
    "\n",
    "The discrete output variable $\\mathcal{y}$ is often also called the **label** or **target** or **class**.\n",
    "\n",
    "For example, we might want to predict whether a patient has a disease or not, based on their symptoms. In this case, $\\mathcal{y}$ is a binary variable, taking the value 1 if the patient has the disease, and 0 otherwise. Other examples of classification problems include predicting the sentiment of a movie review: positive, negative, or neutral.\n",
    "\n",
    "For example,\n",
    "\n",
    "<img align=\"center\" width=\"90%\" src=\"../assets/sentiment.png\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "In other words, the classification problem is to learn a function $f$ that maps the input $\\mathcal{X}$ to the discrete output $\\mathcal{Y}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The most common metric for evaluating a classifier is **accuracy**. Accuracy is the proportion of correct predictions. It is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "$$Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%.\n",
    "\n",
    "Assuming the categorical variable that we are trying to predict is binary, we can define the accuracy in terms of the four possible outcomes of a binary classifier: \n",
    "\n",
    "1. True Positive (TP): The classifier correctly predicted the positive class.\n",
    "2. False Positive (FP): The classifier **incorrectly** predicted the negative class as positive.\n",
    "3. True Negative (TN): The classifier correctly predicted the negative class.\n",
    "4. False Negative (FN):  The classifier **incorrectly** predicted the positive class as negative.\n",
    "\n",
    "True positive means that the classifier correctly predicted the positive class. False positive means that the classifier incorrectly predicted the positive class. True negative means that the classifier correctly predicted the negative class. False negative means that the classifier incorrectly predicted the negative class.\n",
    "\n",
    "These definitions are summarized in the table below: \n",
    "\n",
    "|       | Prediction $\\hat{y} = f\\prime(x)$ | Truth $y = f(x)$     |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| True Negative (TN)    | 0        | 0   |\n",
    "| False Negative (FN)   | 0        | 1      |\n",
    "| False Positive (FP)   | 1        | 0      |\n",
    "| True Positive (TP)   | 1        | 1      |\n",
    "\n",
    "In terms of the four outcomes above, the accuracy is:\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Accuracy is a useful metric, but it can be misleading. \n",
    "\n",
    "Other metrics that are often used to evaluate classifiers are: \n",
    "\n",
    "* **Precision**: The proportion of positive predictions that are correct. Mathematically, precision is defined as:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**: The proportion of positive instances that are correctly predicted. Mathematically, recall is defined as:\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "The precision and recall are often combined into a single metric called the **F1 score**. The F1 score is the harmonic mean of precision and recall. The harmonic mean of two numbers is given by:\n",
    "\n",
    "$$\\frac{2}{\\frac{1}{x} + \\frac{1}{y}}$$\n",
    "\n",
    "* **F1 Score**: The harmonic mean of precision and recall.\n",
    "\n",
    "$$F1\\ Score = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$$\n",
    "\n",
    "<!-- $$Baseline\\ Accuracy = \\frac{Number\\ of\\ majority\\ class\\ predictions}{Total\\ number\\ of\\ predictions}$$ -->\n",
    "\n",
    "<!-- The baseline accuracy is the accuracy of the majority class classifier. It is the accuracy we would get if we just guessed the majority class for every instance. It is a useful baseline to compare our classifier to. If our classifier is not better than the baseline, then we should probably just use the baseline classifier.\n",
    "\n",
    "Another way to evaluate a classifier is to look at the confusion matrix. A confusion matrix is a table that shows the number of correct and incorrect predictions for each class. For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%. But if we had just guessed the majority class for all of them, we would have gotten 50% accuracy. This is called the baseline accuracy.\n",
    "\n",
    "<img src=\"../assets/confusion_matrix.png\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/classification.png\">\n",
    "\n",
    "\n",
    "<img src=\"../assets/cross_validation.png\">\n",
    "\n",
    "\n",
    "<img src=\"../assets/training_testing.png\">\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method for classifying text is to use handwritten rules. There are many areas of language processing where handwritten rule-based classifiers constitute a state-of-the-art system, or at least part of it.\n",
    "\n",
    "We focus on one common text categorization task, sentiment analysis traction of sentiment, the positive or negative orientation that a writer expresses toward some object. A review of a movie, book, or product on the web expresses the author’s sentiment toward the product, while an editorial or political text expresses sentiment toward a candidate or political action. Extracting consumer or public sentiment is thus relevant for fields from marketing to politics.\n",
    "\n",
    "**Spam detection** is another important commercial application, the binary classification task of assigning an email to one of the two classes spam or not-spam.\n",
    "\n",
    "Many lexical and other features can be used to perform this classification. For example you might quite reasonably be suspicious of an email containing phrases like “online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.\n",
    "\n",
    "Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification in language processing are instead done via supervised machine learning, and this will be the subject of the remainder of this chapter. \n",
    "\n",
    "Formally, the task of supervised classification is to take an input x and a fixed set of output classes $Y = {y_1, y_2,..., y_M}$ and return a predicted class $y ∈ Y$. For text classification, we’ll sometimes talk about c (for “class”) instead of y as our output variable, and d (for “document”) instead of x as our input variable. In the\n",
    "supervised situation we have a training set of N documents that have each been hand labeled with a class: ${(d1, c1),....,(dN, cN)}$. \n",
    "\n",
    "Our goal is to learn a classifier that is capable of mapping from a new document d to its correct class c ∈ C, where C is some set of useful document classes. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining system\n",
    "\n",
    "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
