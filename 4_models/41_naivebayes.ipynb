{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes' is a probabilistic model based on the Bayes Theorem (hence the name), used in a wide variety of classification tasks.\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "Recall Bayes' Theorem from probability theory:\n",
    "\n",
    "$$ Posterior = \\frac{Likelihood \\cdot Prior}{Evidence} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(y|X) = \\frac{P(X|y)P(y)}{P(X)} $$\n",
    "\n",
    "Here $X$ is multi-dimensional input data (say Bag of Words representation of a set of documents) whereas $y$ is the corresponding set of labels (say each document's sentiment). \n",
    "\n",
    "<img align=\"center\" src=\"../assets/sentiment.png\" width=\"80%\">\n",
    "\n",
    "## Naive Assumption\n",
    "\n",
    "Note that the likelihood term $P(X|y)$ in the Bayes theorem can get very complicated to compute, for high dimensional data where X may be composed of many features.\n",
    "\n",
    "Naive Bayes' makes a **simplifying naive assumption** that the features are independent of each other, given the class. In other words, \n",
    "\n",
    "$$ P(X|y) = \\prod_{i=1}^{D} P(x_i|y) $$\n",
    "\n",
    "<img src=\"../assets/naive_assumption.png\" width=\"50%\" align=\"center\">\n",
    "\n",
    "This is a naive assumption, since in most cases, features are not independent. This is especially not true for text data, where the presence of a word in a document is highly correlated with the presence of other words in the document. Hence the popular adage in NLP: _\"You shall know a word by the company it keeps\"_ by Firth.\n",
    "\n",
    "However, the naive assumption makes the computation of the likelihood term tractable and still remarkably yields great results in practice.\n",
    "\n",
    "Naive Bayes model is a **generative model** since it makes an assumption about how the data is generated. It assumes that the data is generated by first sampling a class $y$ from the prior distribution $P(y)$ and then sampling each feature $x_i$ from the likelihood distribution $P(x_i|y)$.\n",
    "\n",
    "## Bag of Words (BOW) as Multinomial Data\n",
    "\n",
    "Recall that multinomial distribution counts the number of times an outcome occurs when there are k-possible outcomes and N independent trials. Also recall that BOW representation of text is a count of how many times a word occurs in a document.\n",
    "\n",
    "BOW representation of text is a multinomial data, where each word in the vocabulary is a possible outcome and the length of the document is the number of trials.\n",
    "\n",
    "So we can use the multinomial distribution to model the likelihood term $P(x_i|y)$ in the prediction rule. The parameters of the multinomial distribution are estimated using Maximum Likelihood Estimation (MLE). \n",
    "\n",
    "The Multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. \n",
    "\n",
    "It has multinomial in the name because it assumes that the features have a multinomial distribution. \n",
    "\n",
    "We can use Maximum Likelihood Estimation (MLE) to estimate the parameters of the model.\n",
    "\n",
    "## Multinomial Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes classifier is simply a **probabilistic classifier** where the prediction $\\hat{y}$ is the class $y$ that maximizes the posterior probability $P(y|X)$ i.e. \n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(y|X) $$\n",
    "\n",
    "In other words, if y=0 maximizes P(y | X), then the predicted class is 0. Otherwise, if y=1 maximizes P(y | X), then the predicted class is 1.\n",
    "\n",
    "P(y | X) is proportional to P(X | y)P(y), as per the Bayes' theorem.  So, we can also write the prediction rule as:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(X|y)\\cdot P(y) $$\n",
    "\n",
    "If we make the naive assumption that the features are independent of each other, given the class, then we can write the prediction rule as:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} \\prod_{i=1}^{D} P(x_i|y) \\cdot P(y)  $$\n",
    "\n",
    "Based on our conversation on logarithms, we can also write the prediction rule as:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} \\sum_{i=1}^{D} \\log P(x_i|y) + \\log P(y)  $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
